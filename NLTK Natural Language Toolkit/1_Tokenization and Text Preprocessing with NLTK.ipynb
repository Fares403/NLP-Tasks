{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit (NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit (NLTK) is a powerful Python library that aids in natural language processing tasks. It was developed at the University of Pennsylvania and has become one of the most popular and widely used libraries in NLP.\n",
    "\n",
    "NLTK provides a wide range of functionalities and resources for text processing, tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and much more. It also includes various corpora, lexical resources, and pre-trained models to help you get started quickly.\n",
    "\n",
    "To begin using NLTK in your Python environment, you need to install it first. The installation process is straightforward and can be done using pip, the standard package manager for Python. Open your command prompt or terminal and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing NLTK, you may want to download additional resources like corpora or models depending on your requirements. NLTK provides a convenient way to download these resources using the nltk.download() function.\n",
    "\n",
    "To download all the available resources at once, you can run:\n",
    "Note: Alternatively, you can choose specific resources to download by replacing 'all' with their respective identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Text Preprocessing with NLTK and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTKâ€™s word tokenization allows you to split text into individual words or tokens. This process is essential for analyzing the linguistic structure of a sentence and extracting meaningful information from it. NLTK provides different tokenization methods, including the default word_tokenize() function and alternative options like TreebankWordTokenizer and RegexpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Emy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Emy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Emy\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK resources (only required once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"Tokenization is an important step in Natural Language Processing (NLP). It breaks down text into smaller units called tokens. These tokens can be words, sentences, or even characters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens:\n",
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'It', 'breaks', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', '.', 'These', 'tokens', 'can', 'be', 'words', ',', 'sentences', ',', 'or', 'even', 'characters', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization - Word Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\")\n",
    "print(tokens)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "['Tokenization is an important step in Natural Language Processing (NLP).', 'It breaks down text into smaller units called tokens.', 'These tokens can be words, sentences, or even characters.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization - Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\")\n",
    "print(sentences)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after removing stop words:\n",
      "['Tokenization', 'important', 'step', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'breaks', 'text', 'smaller', 'units', 'called', 'tokens', '.', 'tokens', 'words', ',', 'sentences', ',', 'even', 'characters', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing - Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.casefold() not in stop_words]\n",
    "print(\"Tokens after removing stop words:\")\n",
    "print(filtered_tokens)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens:\n",
      "['token', 'import', 'step', 'natur', 'languag', 'process', '(', 'nlp', ')', '.', 'break', 'text', 'smaller', 'unit', 'call', 'token', '.', 'token', 'word', ',', 'sentenc', ',', 'even', 'charact', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing - Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\")\n",
    "print(stemmed_tokens)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens:\n",
      "['Tokenization', 'important', 'step', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'break', 'text', 'smaller', 'unit', 'called', 'token', '.', 'token', 'word', ',', 'sentence', ',', 'even', 'character', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing - Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\")\n",
    "print(lemmatized_tokens)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after handling special characters:\n",
      "['Tokenization', 'is', 'an', 'important', 'step', 'in', 'Natural', 'Language', 'Processing', 'NLP', 'It', 'breaks', 'down', 'text', 'into', 'smaller', 'units', 'called', 'tokens', 'These', 'tokens', 'can', 'be', 'words', 'sentences', 'or', 'even', 'characters']\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing - Handling Special Characters\n",
    "special_chars = set(string.punctuation)\n",
    "filtered_tokens = [token for token in tokens if token not in special_chars]\n",
    "print(\"Tokens after handling special characters:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK: NNP\n",
      "provides: VBZ\n",
      "powerful: JJ\n",
      "tools: NNS\n",
      "for: IN\n",
      "performing: VBG\n",
      "POS: NNP\n",
      "tagging: NN\n",
      ".: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Emy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources (only required once)\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"NLTK provides powerful tools for performing POS tagging.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print the POS tags\n",
    "for token, pos_tag in pos_tags:\n",
    "    print(f\"{token}: {pos_tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
